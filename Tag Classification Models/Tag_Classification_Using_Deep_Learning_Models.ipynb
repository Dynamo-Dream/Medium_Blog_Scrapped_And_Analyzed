{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Tag Classification Using Deep Learning Models"
      ],
      "metadata": {
        "id": "L5G06Y0T93iI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i62nW1buAa9W"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"MediumDataAfterEDA.csv\")\n",
        "df = data[[\"Paragraph\",\"Tag\"]].copy()"
      ],
      "metadata": {
        "id": "QFPpdJMqAvl3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={\"Paragraph\": \"Blog\"})"
      ],
      "metadata": {
        "id": "syysD1zfA9LN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have 155 Blogs only, I will devide the Blogs into a 80 words with 20 words overlapping in each sentence with their corresponding tags intact to increase the dataset"
      ],
      "metadata": {
        "id": "Wn5OUJmLYr7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "def create_overlapping_segments(text, chunk_size=80, overlap=20):\n",
        "  \"\"\"\n",
        "    Function to create overlapping segments of text with specified chunk size and overlap.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input text to be segmented.\n",
        "        chunk_size (int): The desired chunk size in terms of number of words.\n",
        "        overlap (int): The overlap between adjacent chunks in terms of number of words.\n",
        "\n",
        "    Returns:\n",
        "        List of overlapping segments.\n",
        "  \"\"\"\n",
        "  tokens = word_tokenize(text)\n",
        "  segments = []\n",
        "  start = 0\n",
        "  end = chunk_size\n",
        "  while start < len(tokens):\n",
        "    segment = tokens[start:end]\n",
        "    segments.append(\" \".join(segment))\n",
        "    start += chunk_size - overlap\n",
        "    end = start + chunk_size\n",
        "  return segments\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79vwSw8pBBL3",
        "outputId": "4fbf5d58-449b-4d77-a816-20d9780c73af"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "Y = []\n",
        "for i in range(len(df[\"Blog\"])):\n",
        "  segment =  create_overlapping_segments(df[\"Blog\"][i])\n",
        "  for sentence in segment:\n",
        "    X.append(sentence)\n",
        "    Y.append(df[\"Tag\"][i])"
      ],
      "metadata": {
        "id": "I9ILMBExBfiJ"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi6MjzpCBh7-",
        "outputId": "5fdac12a-e6d3-4bed-9cb2-450625994046"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing steps :\n",
        "1 ) First removing punctuation and html tags if any. note that the html tas may be present ast the data must be scraped from net.\n",
        "\n",
        "2) Tokenize the reviews into tokens or words .\n",
        "\n",
        "3) Next remove the stop words and shorter words as they cause noise.\n",
        "\n",
        "4) Stem or lemmatize the words depending on what does better. Herer I have yse lemmatizer."
      ],
      "metadata": {
        "id": "-Qgx1aR8YkiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def clean_text(text):\n",
        "  sentence = text.lower()\n",
        "  sentence = re.sub(\"[^a-z0-9]\",' ',sentence)\n",
        "  sentence = sentence.split()\n",
        "  sentence = [lemmatizer.lemmatize(word) for word in sentence if word not in STOPWORDS]\n",
        "  sentence = \" \".join(sentence)\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "gv1XoFuzBsYo"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.DataFrame({\"Blog\": X, \"Tag\": Y})"
      ],
      "metadata": {
        "id": "-p3XIyd9BwfP"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[\"Blog\"] = new_df[\"Blog\"].apply(clean_text)"
      ],
      "metadata": {
        "id": "mB0Y8KzMCDyP"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from gensim.models import Word2Vec\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "iIVyi8n5COEX"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Word2Vec Model on my Own Corpus"
      ],
      "metadata": {
        "id": "_ouIag9zZn21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "for doc in new_df[\"Blog\"]:\n",
        "  word = doc.split()\n",
        "  words.append(word)"
      ],
      "metadata": {
        "id": "cZTxfMqJCtV0"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = Word2Vec(sentences=words, vector_size=300, window=5, min_count=1,epochs=15)"
      ],
      "metadata": {
        "id": "23eppsiWCZCA"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.corpus_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edHo6NuaGBOB",
        "outputId": "78ba0c96-1207-4686-95e0-43f977ed732a"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7191"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.wv.similar_by_word('sleep')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egodVapEIENc",
        "outputId": "276d258e-068f-4e5f-f330-7b64cad1ca56"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('likelihood', 0.9140306115150452),\n",
              " ('observed', 0.8850325345993042),\n",
              " ('grows', 0.8681285977363586),\n",
              " ('span', 0.8631398677825928),\n",
              " ('asleep', 0.8579186201095581),\n",
              " ('estimated', 0.8543052077293396),\n",
              " ('measurement', 0.8522398471832275),\n",
              " ('bedroom', 0.8469898104667664),\n",
              " ('credible', 0.8426560759544373),\n",
              " ('variability', 0.8386530876159668)]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.wv['sleep']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s3ifXXGa4Rj",
        "outputId": "773639fb-87d3-498c-c3c4-8b1ba0ff6ab7"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-3.70772220e-02,  2.78305084e-01, -4.30310398e-01,  4.66126613e-02,\n",
              "       -3.43960911e-01, -8.49517465e-01,  5.31706333e-01,  6.14339828e-01,\n",
              "       -2.41224673e-02, -3.12504292e-01, -9.52668265e-02,  1.24070719e-01,\n",
              "       -3.25106472e-01, -2.68386573e-01,  2.76851542e-02, -3.47611755e-02,\n",
              "        2.17228811e-02,  2.23206028e-01,  4.60965097e-01, -1.12694707e-02,\n",
              "        5.65774832e-03,  4.50185873e-02,  2.20357955e-01,  5.50502762e-02,\n",
              "        3.62800658e-02, -2.00003847e-01,  1.39758781e-01, -1.33429185e-01,\n",
              "       -2.28759214e-01, -3.70532632e-01, -6.75683783e-04, -1.25981599e-01,\n",
              "        5.24161160e-02,  6.43315201e-04,  1.74346287e-03,  2.36083895e-01,\n",
              "        1.48884296e-01, -2.81826258e-01,  5.13244048e-02, -4.98542078e-02,\n",
              "        3.68328869e-01,  7.41518214e-02,  2.65826792e-01,  4.40521426e-02,\n",
              "        3.34167659e-01,  2.54520535e-01,  5.68871647e-02,  2.31123775e-01,\n",
              "        1.08772494e-01,  3.90704989e-01,  1.20209657e-01, -4.38411906e-02,\n",
              "        5.52475899e-02,  1.46414891e-01, -2.29405984e-01,  2.58844286e-01,\n",
              "       -1.38492942e-01,  6.38929531e-02,  1.96797431e-01,  1.10363156e-01,\n",
              "       -1.69187322e-01, -1.96533829e-01, -3.94681185e-01, -1.75736681e-01,\n",
              "        2.38202065e-01, -4.65503447e-02,  2.06462577e-01,  8.91639106e-03,\n",
              "        2.79806945e-02,  4.59723473e-02, -4.89354968e-01, -2.80657783e-02,\n",
              "        2.29801372e-01, -1.09976590e-01,  8.12124312e-02,  4.76365417e-01,\n",
              "       -8.92886296e-02,  3.02429378e-01, -1.66686520e-01,  1.50596753e-01,\n",
              "       -2.27077883e-02, -2.93779671e-01, -1.88284397e-01,  3.00082237e-01,\n",
              "        5.02324164e-01,  2.42939338e-01, -1.19802110e-01, -2.50720173e-01,\n",
              "        2.55270928e-01, -4.46984261e-01,  1.15178160e-01,  6.99282661e-02,\n",
              "        2.29139373e-01,  5.46065629e-01,  2.72888869e-01,  3.25005174e-01,\n",
              "        2.54283339e-01, -1.39277756e-01, -1.01035066e-01,  1.28767326e-01,\n",
              "        6.11372590e-02,  5.05232394e-01,  4.95219409e-01,  1.31024106e-03,\n",
              "        1.81976676e-01, -1.57751143e-01, -9.35622454e-02,  3.11670035e-01,\n",
              "       -2.41675600e-01,  1.27876982e-01, -2.86855787e-01,  8.80440623e-02,\n",
              "       -2.01268613e-01,  5.55542484e-02,  1.67133331e-01, -2.96840847e-01,\n",
              "       -7.46106058e-02,  1.72898434e-02,  1.77064523e-01, -5.01814723e-01,\n",
              "       -6.41969442e-02, -3.52606848e-02,  6.91993237e-02, -4.55490738e-01,\n",
              "       -5.57532549e-01,  2.09210634e-01,  5.95048606e-01, -3.75294507e-01,\n",
              "        2.38794550e-01,  3.99225466e-02,  8.25420395e-02,  3.64895612e-01,\n",
              "        1.40777498e-01,  5.38137034e-02,  5.25034815e-02,  2.50361204e-01,\n",
              "        6.56037852e-02, -1.25173017e-01, -1.00855201e-01, -3.31654638e-01,\n",
              "       -6.58073127e-02, -2.37009659e-01,  1.53350592e-01,  5.26106000e-01,\n",
              "        6.93616197e-02,  1.11470535e-01, -8.98056507e-01, -1.77212089e-01,\n",
              "        3.22580963e-01, -8.50880519e-03,  5.35967201e-02, -1.79904372e-01,\n",
              "        5.56374528e-02, -3.43782008e-01, -4.86327440e-01,  3.88438284e-01,\n",
              "       -4.82256025e-01,  1.36099756e-03, -9.04173329e-02,  1.28230490e-02,\n",
              "        6.48040771e-02,  3.11180443e-01, -5.96396685e-01,  3.65039170e-01,\n",
              "        9.05929320e-03, -3.01254213e-01,  2.88647234e-01,  1.46675274e-01,\n",
              "        3.55363786e-02,  1.34078264e-01, -1.06504075e-02,  3.02908808e-01,\n",
              "        7.91877359e-02, -2.29384489e-02,  1.56892106e-01, -1.39876232e-01,\n",
              "       -1.42603666e-01, -2.46974081e-01,  9.25509483e-02,  1.29076587e-02,\n",
              "        1.26564605e-02,  1.66592121e-01,  3.66636142e-02,  4.65872362e-02,\n",
              "        5.47109485e-01,  1.65756702e-01,  2.73262352e-01,  8.34171772e-02,\n",
              "        4.92645770e-01,  8.15295577e-02,  9.83344764e-02,  1.53274149e-01,\n",
              "       -5.83780885e-01, -2.62923032e-01,  7.74752647e-02,  1.80230290e-01,\n",
              "        2.64836669e-01, -3.00004870e-01,  5.73132858e-02, -2.44890139e-01,\n",
              "       -2.94190884e-01,  7.18323141e-02,  1.80271849e-01, -1.29844174e-01,\n",
              "        2.52151161e-01, -3.44168246e-01, -6.15391359e-02, -1.03592061e-01,\n",
              "       -7.62175545e-02, -1.21770471e-01,  3.51936609e-01, -8.77805650e-02,\n",
              "       -3.82826895e-01, -5.12695685e-02, -6.81689531e-02, -3.75228345e-01,\n",
              "       -3.35468292e-01, -2.54683763e-01, -2.31038436e-01, -3.11768800e-01,\n",
              "        4.94022369e-02,  2.25787342e-01,  9.14932787e-02, -1.98518082e-01,\n",
              "       -1.24762900e-01, -4.45795625e-01,  1.73526853e-02, -4.00364906e-01,\n",
              "       -2.04341277e-01,  7.00567290e-02,  1.28297016e-01,  2.74954457e-02,\n",
              "        9.67427641e-02,  2.49707386e-01, -2.09247127e-01,  4.08077329e-01,\n",
              "        1.88601404e-01,  9.80856121e-02, -2.36053959e-01, -2.15612277e-01,\n",
              "        7.75462538e-02, -2.93269575e-01, -4.74535346e-01,  3.17350775e-01,\n",
              "       -1.02895528e-01, -3.85748684e-01, -2.89157722e-02, -3.36016357e-01,\n",
              "       -1.61709905e-01,  2.85358191e-01, -2.88565338e-01, -8.57966766e-03,\n",
              "       -5.78467362e-02, -1.45488918e-01, -3.66287142e-01, -3.34945135e-02,\n",
              "        2.16358796e-01, -9.24604759e-02, -4.53285396e-01, -9.97735783e-02,\n",
              "        7.50748515e-01,  1.50516585e-01,  1.89729571e-01, -7.60381520e-01,\n",
              "       -3.48033756e-01, -1.97601289e-01,  6.12917542e-01,  1.89454615e-01,\n",
              "       -1.06652051e-01,  1.31092519e-01, -3.94752175e-01, -1.29525393e-01,\n",
              "       -6.71303496e-02, -1.21171094e-01,  5.18876314e-01, -1.14665506e-03,\n",
              "       -2.38452643e-01,  3.82741481e-01, -1.41735345e-01,  2.20556021e-01,\n",
              "        4.21357065e-01,  2.84567744e-01,  9.47822332e-02,  1.77021131e-01,\n",
              "        1.92534775e-01, -4.86980021e-01, -3.86843711e-01, -2.48266071e-01,\n",
              "        1.32247388e-01,  4.09424961e-01,  1.93098351e-01,  1.99211583e-01,\n",
              "        3.35553914e-01,  1.78369641e-01,  6.11897886e-01,  3.32410723e-01,\n",
              "       -1.65791646e-01, -1.87385663e-01,  2.81673372e-01, -2.45031685e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxi=-1\n",
        "for i,sen in enumerate(new_df['Blog']):\n",
        "    tokens=sen.split()\n",
        "    if(len(tokens)>maxi):\n",
        "        maxi=len(tokens)"
      ],
      "metadata": {
        "id": "FdO-bCK98MCf"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now integer encode the words in the reviews using Keras tokenizer."
      ],
      "metadata": {
        "id": "3ybyKjEyb6ZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_sequences = tokenizer.texts_to_sequences(X)"
      ],
      "metadata": {
        "id": "ZxjdzJKA8lQm"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_padded= pad_sequences(X_sequences, maxlen=maxi, padding='post')\n",
        "X_padded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHPMdOnHbqKv",
        "outputId": "383322de-5ed3-4bce-fa70-10c10ea01a5f"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7191, 77)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to pass the w2v word embeddings to the embedding layer in Keras. For this lets create the embedding matrix and pass it as 'embedding_initializer' parameter to the layer."
      ],
      "metadata": {
        "id": "cOMPnTRtcT6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embed_dim=300\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8fdPuVDcMmG",
        "outputId": "744c0afe-80c2-49a2-c967-4303caa6713a"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13303"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix=np.zeros(shape=(vocab_size,embed_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if word in word2vec_model.wv:\n",
        "    embedding_matrix[i] = word2vec_model.wv[word]"
      ],
      "metadata": {
        "id": "1-Wwlyeq9oqF"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix[1]"
      ],
      "metadata": {
        "id": "3ZOibrWc7d6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a689d8-c118-440d-d45d-5e264a482cd0"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "encoded_tags = label_encoder.fit_transform(new_df['Tag'])\n",
        "Y=to_categorical(encoded_tags)\n",
        "Y[0]"
      ],
      "metadata": {
        "id": "hLZAAUjd8DXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c93122a-caa3-4ccb-8a7c-b9a475cf1c83"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X_padded, Y)"
      ],
      "metadata": {
        "id": "XDkZ4kcd_fxY"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "UL_1nrx68TqQ"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, LSTM, Dense, SpatialDropout1D\n",
        "from keras.initializers import Constant\n",
        "from keras.layers import Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
        "                    output_dim=word2vec_model.vector_size,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=maxi,\n",
        "                    trainable=False))\n",
        "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(Y.shape[1], activation='softmax'))"
      ],
      "metadata": {
        "id": "WyXBFbcE9rsK"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLmwmGsPg7PL",
        "outputId": "27ae4502-df77-4e1f-9268-c7e3c32cec58"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13624, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHuDQpftjM2E",
        "outputId": "cd8b7040-5dad-4934-dd45-22776ba57262"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13624, 77)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz0I8PRgM7cS",
        "outputId": "94fa7602-454d-4095-c49c-817c9c5ddd06"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (None, 77, 300)           3990900   \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 64)                93440     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4084730 (15.58 MB)\n",
            "Trainable params: 93830 (366.52 KB)\n",
            "Non-trainable params: 3990900 (15.22 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWffqoIe9j-z",
        "outputId": "cb1fe28d-b512-46b6-e560-6665628fedc8"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "192/192 [==============================] - 50s 237ms/step - loss: 1.5274 - accuracy: 0.3485 - val_loss: 1.3160 - val_accuracy: 0.4204\n",
            "Epoch 2/5\n",
            "192/192 [==============================] - 46s 242ms/step - loss: 1.2967 - accuracy: 0.4334 - val_loss: 1.2516 - val_accuracy: 0.4578\n",
            "Epoch 3/5\n",
            "192/192 [==============================] - 47s 243ms/step - loss: 1.2273 - accuracy: 0.4623 - val_loss: 1.2542 - val_accuracy: 0.4519\n",
            "Epoch 4/5\n",
            "192/192 [==============================] - 50s 259ms/step - loss: 1.1725 - accuracy: 0.4901 - val_loss: 1.1598 - val_accuracy: 0.4879\n",
            "Epoch 5/5\n",
            "192/192 [==============================] - 49s 256ms/step - loss: 1.1377 - accuracy: 0.4994 - val_loss: 1.1339 - val_accuracy: 0.4923\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 1.1326 - accuracy: 0.4908\n",
            "Test Accuracy: 0.4907529652118683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
        "                    output_dim=word2vec_model.vector_size,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=maxi,\n",
        "                    trainable=False))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(Y.shape[1], activation='softmax'))"
      ],
      "metadata": {
        "id": "yC8y-SHo9kFr"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZIYaevJluuZ",
        "outputId": "b455b4b0-6ea0-4b89-a6ca-f0c630912cba"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, 77, 300)           3990900   \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 75, 64)            57664     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 37, 64)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2368)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 64)                151616    \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4200570 (16.02 MB)\n",
            "Trainable params: 209670 (819.02 KB)\n",
            "Non-trainable params: 3990900 (15.22 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4ADeaW7lyxB",
        "outputId": "338283b9-0d22-4db6-d5f8-fb6fb3b18c20"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "192/192 [==============================] - 20s 99ms/step - loss: 1.5584 - accuracy: 0.3538 - val_loss: 1.3927 - val_accuracy: 0.4226\n",
            "Epoch 2/5\n",
            "192/192 [==============================] - 13s 69ms/step - loss: 1.2683 - accuracy: 0.4810 - val_loss: 1.2600 - val_accuracy: 0.4666\n",
            "Epoch 3/5\n",
            "192/192 [==============================] - 9s 46ms/step - loss: 1.0876 - accuracy: 0.5602 - val_loss: 1.2408 - val_accuracy: 0.4732\n",
            "Epoch 4/5\n",
            "192/192 [==============================] - 9s 49ms/step - loss: 0.9496 - accuracy: 0.6221 - val_loss: 1.2720 - val_accuracy: 0.4652\n",
            "Epoch 5/5\n",
            "192/192 [==============================] - 9s 48ms/step - loss: 0.8092 - accuracy: 0.6878 - val_loss: 1.3153 - val_accuracy: 0.4791\n",
            "48/48 [==============================] - 1s 15ms/step - loss: 1.4230 - accuracy: 0.4551\n",
            "Test Accuracy: 0.4550858736038208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, GRU, Dense\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
        "                    output_dim=word2vec_model.vector_size,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=maxi,\n",
        "                    trainable=False))\n",
        "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "model.add(GRU(32, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(Y.shape[1], activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT9yxlnGl3Hi",
        "outputId": "36ea79b9-dccc-4fe0-9385-52479c0ffe52"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_11 (Embedding)    (None, 77, 300)           3990900   \n",
            "                                                                 \n",
            " lstm_12 (LSTM)              (None, 77, 32)            42624     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 32)                6336      \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 6)                 198       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4040058 (15.41 MB)\n",
            "Trainable params: 49158 (192.02 KB)\n",
            "Non-trainable params: 3990900 (15.22 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "192/192 [==============================] - 102s 266ms/step - loss: 1.5395 - accuracy: 0.3396 - val_loss: 1.3073 - val_accuracy: 0.4197\n",
            "Epoch 2/5\n",
            "192/192 [==============================] - 52s 273ms/step - loss: 1.2936 - accuracy: 0.4381 - val_loss: 1.2230 - val_accuracy: 0.4424\n",
            "Epoch 3/5\n",
            "192/192 [==============================] - 51s 264ms/step - loss: 1.2194 - accuracy: 0.4625 - val_loss: 1.2113 - val_accuracy: 0.4718\n",
            "Epoch 4/5\n",
            "192/192 [==============================] - 51s 263ms/step - loss: 1.1842 - accuracy: 0.4812 - val_loss: 1.1703 - val_accuracy: 0.4732\n",
            "Epoch 5/5\n",
            "192/192 [==============================] - 50s 263ms/step - loss: 1.1439 - accuracy: 0.4934 - val_loss: 1.1444 - val_accuracy: 0.4784\n",
            "48/48 [==============================] - 2s 42ms/step - loss: 1.1414 - accuracy: 0.4808\n",
            "Test Accuracy: 0.48084545135498047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "usXJduAgmfLb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}