{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tag Classification using ML Algorithms"
      ],
      "metadata": {
        "id": "OPZIHH-r-AZe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "pYGDLiq8QC1g"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "0Bh88ApgRqcX"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"MediumDataAfterEDA.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "7Beo7bC4R2uV",
        "outputId": "14c336ee-01f4-4101-cb58-b6c41ff425b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Blog           Tag\n",
              "0  In October 2020, I was interviewed by DrivenDa...  Data Science\n",
              "1  Talking is a lot like writing in that it force...  Data Science"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-43063995-adf6-4726-93c6-6c90f723f102\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Blog</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In October 2020, I was interviewed by DrivenDa...</td>\n",
              "      <td>Data Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Talking is a lot like writing in that it force...</td>\n",
              "      <td>Data Science</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43063995-adf6-4726-93c6-6c90f723f102')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-43063995-adf6-4726-93c6-6c90f723f102 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-43063995-adf6-4726-93c6-6c90f723f102');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-62162a5e-fe17-495d-be34-3f016f61cc78\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-62162a5e-fe17-495d-be34-3f016f61cc78')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-62162a5e-fe17-495d-be34-3f016f61cc78 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 155,\n  \"fields\": [\n    {\n      \"column\": \"Blog\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 155,\n        \"samples\": [\n          \"How best to describe a Kaggle contest? It\\u2019s a machine learning education disguised as a competition! Although there are valid criticisms of Kaggle, overall, it\\u2019s a great community that provides interesting problems, thousands of data scientists willing to share their knowledge, and an ideal environment for exploring new ideas. As evidence of this, I never would have learned about the Gradient Boosting Machine, or, one of the topics of this article, automated model optimization, were it not for the Kaggle Home Credit contest. In this article, part three of a series (Part One: Getting Started and Part Two: Improving) documenting my work for this contest, we will focus on a crucial aspect of the machine learning pipeline: model optimization through hyperparameter tuning. In the second article, we decided on the Gradient Boosting Machine as our model of choice, and now we have to get the most out of it through optimization. We\\u2019ll do this primarily with two methods: random search and automated tuning with Bayesian optimization. All the work presented here is available to run on Kaggle in the following notebooks. The article itself will highlight the key ideas but the code details are all in the notebooks (which are free to run with nothing to install!) For this article we will skip the background, so if at any time you feel lost, I encourage you to go to the previous articles and notebooks. All of the notebooks can be run on Kaggle without the need to download anything, so I highly recommend checking them out or competing! In the first part of this series, we got familiar with the dataset, performed exploratory data analysis, tried our hand at feature engineering, and built a few baseline models. Our public leaderboard score from this round was 0.678 (which at the moment would place us lower than 4000 on the leaderboard). Part two involved in-depth manual feature engineering, followed by feature selection and more modeling. Using the expanded (and then contracted) set of features, we ended up with a score of 0.779, quite an improvement from the baseline, but not quite in the top 50% of competitors. In this article, we will once again better our score and move up 1000 places on the leaderboard. Optimization in the context of machine learning means finding the set of model hyperparameter values that yield the highest cross validation score for a given dataset. Model hyperparameters, in contrast to model parameters that are learned during training, are set by the data scientist before training. The number of layers in a deep neural network is a model hyperparameter while the splits in a decision tree are model parameters. I like to think of model hyperparameters as settings that we need to tune for a dataset: the ideal combination of values is different for every problem! There are a handful of ways to tune a machine learning model: These are presented in order of increasing efficiency, with manual search taking the most time (and often yielding the poorest results) and automated methods converging on the best values the quickest, although, as with many topics in machine learning, this is not always the case! As shown in this great paper, random search does surprisingly well (which we\\u2019ll see shortly). (There are also other hyperparameter tuning methods such as evolutionary and gradient-based. There are constantly better methods being developed, so make sure to stay up to date on the current best practice!) In the second part of this series, we decided to use the Gradient Boosting Machine (GBM) model because of its superior performance on structured data with many features. The GBM is extremely powerful (yet easy to implement in Python), but it has dozens of hyperparameters that significantly affect performance and which must be optimized for a problem. If you want to feel overwhelmed, check out the documentation on the LightGBM library: I don\\u2019t think there is anyone in the world who can look at all of these and pick the best values! Therefore, we need to implement one of the four methods for choosing the hyperparameters. For this problem, I didn\\u2019t even try manual tuning, both because this was my first time working with the GBM and because I didn\\u2019t want to waste any time. e\\u2019ll skip straight to random search and automated techniques using Bayesian optimization. In the first notebook, we go through an implementation of grid and random search covering the four parts of an optimization problem: These four parts also form the basis of Bayesian optimization, so laying them out here will help when it comes to that implementation. For the code details, refer to the notebooks, but here we\\u2019ll briefly touch on each concept. The objective function takes in a set of inputs and returns a score that we want to maximize. In this case, the inputs are the model hyperparameters, and the score is the 5-fold cross validation ROC AUC on the training data. The objective function in pseudo-code is: The goal of hyperparameter optimization is to find the hyperparameters that return the best value when passed into the objective function. That seems pretty simple, but the problem is evaluating the objective function is very expensive in terms of time and computational resources. We cannot try every combination of hyperparameter values because we have a limited amount of time, hence the need for random search and automated methods. The domain is the set of values over which we search. For this problem with a GBM, the domain is as follows: We can visualize two of these distributions, the learning rate, which is logarithmic normal, and the number of leaves which is uniform normal: Although we don\\u2019t generally think of them as such, both grid and random search are algorithms. In the case of grid search, we input the domain and the algorithm selects the next value for each hyperparameter in an ordered sequence. The only requirement of grid search is that it tries every combination in a grid once (and only once). For random search, we input the domain and each time the algorithm gives us a random combination of hyperparameter values to try. There are no requirements for random search other than that the next values are selected at random. Random search can be implemented as in the following: This is in effect a very simple algorithm! The results history is a data structure that contains the hyperparameter combinations and the resulting score on the objective function. When we get to Bayesian Optimization, the model actually uses the past results to decide on the next hyperparmeters to evaluate. Random and grid search are uninformed methods that do not use the past history, but we still need the history so we can find out which hyperparameters worked the best! In this case, the results history is simply a dataframe. Automated hyperparameter tuning with bayesian optimization sounds complicated, but in fact it uses the same four parts as random search with the only difference the Algorithm used. For this competition, I used the Hyperopt library and the Tree Parzen Estimator (TPE) algorithm with the work shown in this notebook. For a conceptual explanation, refer to this article, and for an implementation in Python, check out the notebook or this article. The basic concept of Bayesian optimization is that it uses the previous evaluation results to reason about which hyperparameters perform better and uses this reasoning to choose the next values. Hence, this method should spend fewer iterations evaluating the objective function with poorer values. Theoretically, Bayesian optimization can converge on the ideal values in fewer iterations than random search (although random search can still get lucky)! The aspirations of Bayesian optimization are: This is a powerful method that promises to deliver great results. The question is, does the evidence in practice show this to be the case? To answer that, we turn to the final notebook, a deep dive into the model tuning results! After the hard work of implementing random search and Bayesian optimization, the third notebook is a fun and revealing exploration of the results. There are over 35 plots, so if you like visuals, then check it out. Although I was trying to do the whole competition on Kaggle, for these searches, I did 500 iterations of random search and 400 of Bayesian optimization which took about 5 days each on a computer with 64 GB of RAM (thanks to Amazon AWS). All the results are available, but you\\u2019ll need some serious hardware to redo the experiments! First off: which method did better? The image below summarizes the results from 501 iterations of random search and 402 of Bayesian optimization (called opt in the dataframe): Going by max score, random search does slightly better but if we measure by average score, bayesian optimization wins. The good news is this is almost exactly what we expect: random search can happen upon a great set of values because it thoroughly explores the search space, but Bayesian optimization will \\u201cfocus\\u201d on the highest scoring hyperparameter values by reasoning from previous results. Let\\u2019s take a look at a very revealing plot, the value of the cross validation score versus the number of iterations: We see absolutely no trend for random search, while Bayesian Optimization (again shown by opt ) improves over the trials. If you can understand this graph, then you can see the benefits of both methods: random search explores the search domain but Bayesian optimization gets better over time. We can also see that Bayesian optimization appears to reach a plateau, indicating diminishing returns to further trials. Second major question: what were the best hyperparameter values? Below are the optimal results from Bayesian optimization: We can use these results to build a model and submit predictions to the competition, or they can be used to inform further searches by allowing us to define a concentrated search space around the best values. One interesting aspect to consider is the values tried by each search method for each hyperparameter. The plots below show kernel density estimate (KDE) functions for each of the search methods as well as the sampling distribution (the hyperparameter grid). The dashed vertical lines indicate the optimal value found for each method. First is the learning rate: Even though the learning rate distribution stretched over several orders of magnitude, both methods found optimal values quite low in the domain. We can use this knowledge to inform further hyperparameter searches by concentrating our search in this region. In most cases, a lower learning rate increases the cross validation performance but at the cost of increased run-time, which is a trade-off we have to make. Let\\u2019s look at some other graphs. For most of the hyperparameters, the optimal values from both methods are fairly close, but not for colsample_bytree : This refers to the fraction of columns used when building each tree in the GBM, and random search found the optimal value was higher than Bayesian optimization. Again, these results can be used for further searches, as we see that the Bayesian method tended to concentrate on values around 0.65. We\\u2019ll show two more plots from this analysis because they are fairly interesting, involving two regularization parameters: What\\u2019s noteworthy here is that these two hyperparameters appear to be complements of one another: if one regularization value is high, then we want the other to be low and vice versa. Maybe this helps the model achieve a balance between bias/variance, the most common issue in machine learning. While random search does not change the distribution of values over the search, Bayesian optimization does, by concentrating on where it thinks the best values are in the search domain. We can see this by graphing the hyperparameter values against the iteration: The hyperparameters with the clearest trend are colsample_bytree and learning_rate which both continue downward over the trials. The reg_lambda and reg_alpha are diverging, which confirms our earlier hypothesis that we should decrease one of these while increasing the other. We want to be careful about placing too much value in these results, because the Bayesian optimization might have found a local minimum of the cross validation loss that it is exploiting. The trends here are pretty small, but it\\u2019s encouraging that the best value was found close to the end of the search indicating cross validation scores were continuing to improve. These next plots show the value of a single hyperparameter versus the score. We want to avoid placing too much emphasis on these graphs because we are not changing one hyperparameter at a time and there can be complex interactions between multiple hyperparameters. A truly accurate graph would be 10-dimensional and show the values of all hyperparameters and the resulting score. If we could understand a 10-dimensional graph, then we might be able to figure out the optimal combination of hyperparameters! Here the random search is in blue and the Bayesian in green: The only clear distinction is that the score decreases as the learning rate increases. We cannot say whether that is due to the learning rate itself, or some other factor (we will look at the interplay between the learning rate and the number of estimators shortly). There are not any strong trends here. The score versus subsample is a little off because boosting_type = 'goss' cannot use subsample (it must be set equal to 1.0). While we can\\u2019t look at all 10 hyperparameters at one time, we can view two at once if we turn to 3D plots! To try and examine the simultaneous effects of hyperparameters, we can make 3D plots with 2 hyperparameters and the score. A truly accurate plot would be 10-D (one for each hyperparameter) but in this case we will stick to 3 dimensions. (See the code for details, 3D plotting in Python is surprisingly straightforward). First we can show the reg_alpha and reg_lambda , the regularization hyperparameters versus the score (for Bayesian opt): It\\u2019s a little hard to make sense, but if we remember the best scores occurred around 0.5 for reg_alpha and 0.4 for reg_lambda , we can see generally better scores in this region. Next is the learning_rate and n_estimators (number of decision trees trained in the ensemble). While the learning rate was a hyperparameter in the grid, the number of decision trees (also called the number of boosting rounds) was found by early stopping with 5-fold cross validation: This time there is a clear trend: a lower learning rate and higher number of estimators increases the score. The relationship is expected because a lower learning rate means the contribution of each tree is reduced which necessitates training more trees. The expanded number of trees increases the model\\u2019s capacity to fit the training data (while also increasing the time to run). Moreover, as long as we use early stopping with enough folds, we don\\u2019t have to be concerned about overfitting with more trees. It\\u2019s nice when the results agree with our understanding (although we probably learn more when they don\\u2019t agree!) For the final plot, I wanted to show the correlations between each hyperparameter and each other and the score. These plots can\\u2019t prove causation, but they can tell us which variables are correlated: We already found most of the information from the graphs, but we can see the negative correlation between the learning rate and the score, and the positive correlation between the number of estimators and the score. The final step of the exploration was to implement both the random search best hyperparameters and the Bayesian optimization best hyperparameters on a full dataset (the dataset comes from this kernel and I would like to thank the author for making it public). We train the model, make predictions on the testing set, and finally upload to the competition to see how we do on the public leaderboard. After all the hard work, do the results hold up? If we go by best score on the public leaderboard, Bayesian Optimization wins! However, the public leaderboard is based only on 10% of the test data, so it\\u2019s possible this is a result of overfitting to this particular subset of the testing data. Overall, I would say the complete results \\u2014 including cross validation and public leaderboard \\u2014 suggest that both methods produce similar outcomes when run for enough iterations. All we can say for sure is that either method is better than hand-tuning! Our final model is enough to move us 1000 places up the leaderboard compared to our previous work. Finally, to end with one more plot, we can take a look at the feature importances from the trained GBM: NEW_CREDIT_TO_ANNUITY_RATIO and NEW_EXT_SOURCES_MEAN were features derived by the data science community on Kaggle and not in the original data. It\\u2019s reassuring to see these so high up in the importance because it shows the value of feature engineering. The major takeaways from this work are: Where to go from here? Well, there are always plenty of other methods to try such as automated feature engineering or treating the problem as a time-series. I\\u2019ve already done a notebook on automated feature engineering so that will probably be where my focus will turn next. We can also try other models or even journey into the realm of deep learning! I\\u2019m open to suggestions, so let me know on Kaggle or on Twitter. Thanks for reading, and if you want to check out any of my other work on this problem, here are the complete set of notebooks: There should be enough content there to keep anyone busy for a little bit. Now it\\u2019s off to do more learning/exploration for the next post! The best part about data science is that you\\u2019re constantly on the move, looking for the next technique to conquer. Whenever I find out what that is, I\\u2019ll be sure to share it! As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will.\",\n          \"Examining the Doctor\\u2019s Appointment No-Show Dataset Author\\u2019s Note: The following exploratory data analysis project was completed as part of the Udacity Data Analyst Nanodegree that I finished in May 2017. All code for this project can be found on my GitHub repository for the class. I highly recommend the course to anyone interested in data analysis (that is anyone who wants to make sense of the mass amounts of data generated in our modern world) as well as to those who want to learn basic programming skills in an applied setting. This version of the Exploratory Data Analysis project has all the code removed for readability. The version with all the R code included is also on Medium. Doctor\\u2019s appointment no-shows are a serious issue in the public health care field. Missed appointments are associated with poorer patient outcomes and cost the health care system in the US nearly $200 each. Therefore, it comes as no small surprise that reducing the rate of no-shows has become a priority in the United States and around the world. Numerous studies have been undertaken in order to determine the most effective means of\\nreducing rates of absenteeism at with varying degrees of success. The first step to solving the problem of missed appointments is identifying why a patient skips a scheduled visit in the first place. What trends are there among patients with higher absence rates? Are there demographic indicators or perhaps time-variant relationships hiding in the data? Ultimately, it was these questions that drove my exploratory data analysis. I was curious as to the reasons behind missed appointments, and wanted to examine the data to identify any trends present. I choose this problem because I believe it is an excellent example of how data science and analysis can reveal relationships which can be implemented in the real-world to the benefit of society. I wanted to choose a dataset that was both relatable and could be used to make smarter decisions. Therefore, I decided to work with medical appointment no shows data available on Kaggle. This dataset is drawn from 300,000 primary physician visits in Brazil across 2014 and 2015. The information about the appointment was automatically coded when the patient scheduled the appointment and then the patient was marked as having either attended or not. The information about the appointment included demographic data, time data, and conditions concerning the reason for the visit. There were a total of 14 variables I included from the original data. The variables and the description of the values are as follows Let\\u2019s take a look at the structure of the dataframe to identify cleaning/organizing that may need to be performed. From the structure of the dataframe, I can see there is some data housekeeping that needs to be done. First, I want to change the Status variable into an integer 0 or 1. A value of 1 will indicate that the patient did not show up as I am concerned with the variables that are most strongly correlated with a missed appointment. Moreover, I need to convert the registration date and the appointment date to date objects. I decided not to maintain the time (hours:minutes:seconds) associated with the registration date for the appointment although it was available (time of day was not available for the appointment date). I converted both the appointment registration date and appointment date to date objects so I could investigate seasonal and time patterns in the data. Based on the updated structure of the dataframe, it looks like I am on the right track but I still have a couple more modifications to make to the dataframe. I will create month, year, and day fields for the appointment and also rename the columns to a more consistent and readable format. That should about do it for the structure of the main dataframe. I am also concerned with missing/corrupted data, so I will look at the summary of each field to see if any obvious errors or outliers are present in the data. The summary of the no_shows dataframe reveals plenty of intriguing information as well as several errors that need to be corrected. Right away, it is clear that an age of -2 is not correct, and 113 is stretching the boundaries. However, looking at the oldest people in the world, it is plausible that 113 could be a correct age. I will filter out any negative ages and create a histogram of ages to see if there are a significant amount of outliers at the upper end. Moreover, this data summary shows that 30.24% of appointments are missed based on the mean for the status field. The rates for the various patient conditions can also be seen, and the most commonly coded reason for an appointment is hypertension at nearly 22% of visits. Also, the weekends appear to be a very unpopular time for doctor\\u2019s appointments in Brazil, or perhaps the clinics from which this data was drawn are not open on the weekend. Either way, I will need to keep the small sample size of appointments on the weekend in mind when I perform an analysis by weekday. Frequency Distribution of Ages The age histogram does not aligh exactly with my intutions though it is close. I expected that it would be bi-modal, with peaks in patient count at the youngest ages and at the oldest ages. However, while the largest number of patients do appear to be the youngest, the number of patients remains fairly constant into the middle ages, with a second peak around age 60 and then a steep decline into the oldest ages. Based on the visualization, there do not appear to be a large number of outliers in the upper range of the ages. I was skeptical of the high proportion of visits from those aged 0 and decided I needed to do some research. Based on the description of the dataset, this data is for primary care physicians in the public sector and so I looked for a breakdown of ages of patients at these appointments. I could not find statistics from Brazil, but based on official statistics from the Centers for Disease Control in the United States, children under 1 year of age make up 2.6% of all physician visits. I can create a better plot showing the percentage each age is of all patients. I can also quickly check the percentage of visits comprised of patients aged 0 in the data. Percentage of patients with an age of 0:\\n3.442069% The calculation shows that 0-year olds make up 3.44% of visits. While the initial frequency distribution may looked skewed, the spread of the data along the years makes it look like 0-year olds make up considerably more of the visits than is actually the case. The corrected bar plot does a better job of representing the proportion of patients from each age. From the research by the CDC and the visualization, I conclude that the age distribution skew is not bad but rather legitimate data. To check for outliers in the waiting time field (or how long between the date the appointment was made and the actual date of the appointment), I will make another histogram. Based on the summary statistics of the dataframe, the longest waiting time was 398 days, with a median of 8 days, and a mean of 13.84 days. From the histogram and the statistics, it is clear that there are a small number of patients at the upper end of the distribution who schedule their appointment very far in advance. I wonder if the 398 days is accidentally a mistake that occured when someone choose the wrong year for their appointment! The graph is certainly long-tailed and positively skewed with a few extreme outliers at the upper limit. Number of patients with wait time longer than one year:\\n1\\nNumber of patients with wait time longer than six months:\\n136 It appears there is only one patient with a waiting time over one year and just over 100 (out of 300000) with a waiting time of over six months. I will exclude the patient with the waiting time over one year, but I will treat the others as good but extreme data. Here is a version of the histogram that better represents the wait time for most patients. As can be seen in the chart, a plurality of patients wait less than 10 days between the scheduling and the date of their appointment. Based on these visualizations, and the summary statistics for the dataframe, I am confident that the data content of the main dataframe is valid. I can now start the exploration phase of exploratory data analysis! It is time to discover the relationships, or lack thereof, between the variables in the data. The first step I can take is to find the correlations between all of the columns to see if any stand out as particularly compelling. Of course, I expect that all the trends will not reveal themselves straight away. The rcorr function from the Hmisc library will calculate the Pearson\\u2019s correlation coeffient between every field of a dataframe. The Pearson correlation coefficient, is a measure of how linearly dependent two variables on each other. The coefficient value is between -1 and +1, and two variables that are perfectly linearly correlated will have a value of +1. From those correlation values, there are no standouts that are strong linear predictors of whether or not a patient will miss a visit (given by the status column). It appears that there are not even any strong relationships whatsoever though there is a moderate relationship between age and hypertension. However, I still think there are meaningful relationships to extract from the data. My approach will be to group the data by various fields and then determine if the average absence rate shows a relationship with the fields. The first group of variables I will look at will be those associated with the time-variability of the appointment date. Date of Appointment To start off the time-series analysis, I will want to look at absence rate by month of the year. I first needed to group the appointments by month of year and then find the average absence rate for each month. From the graph and the correlation coefficient, it appears there is a minor relationship between month of the year and absence rate. Missed visits do appear to rise as the year progresses, but with only 12 data points, the 95% confidence interval for the correlation coefficient is quite large (-0.05 to 0.85). I would expect that during the end of the year, people tend to be busier and may miss more appointments. It could also be possible that at the beginning of the year, individuals make resolutions to see the doctor and keep a higher percentage of appointments, but the trend could also be noise. Next, I will create a similar absence rate over time graph, but this time by day of the month. Looking at the plot we can see that there is a very slight positive correlation between day of the month and absence rate but again, the 95% confidence interval spans a wide range of values. To further explore any time variability in the data, I can create the same graph by day of the year and broken out by gender. Mean absence rate for males:\\n30.99653%\\nMean absence rate for females\\n29.86962% From these charts, it appears looks as if there is no correlation betwen the time of year and the absence rate for appointments. Moreover, adding in the gender does not reveal any major descrepancies, although from the calculation, we can see that men on average have a 1% higher absence rate. There are several days for which the absence rate is 1, which bears some more investigation. My idea is that these points might corespond to public holidays. However, I also see that these days are not consistent across the two years, so maybe there are other major events that correspond to an increase in absences. I decided to overlay the holidays to see if that might reveal a relationship. I sourced the holidays based on public holidays in Brazil for each respective year. Again, it seems as if these holidays do not line up with higher rates of missed appointments. There is a sustained increase in the absence rate around the beginning of July, but it does not seem to correspond to any holidays. I researched festivals in Brazil in June-July 2015, and it appears that the Paraty International Literary Festival occured July 1\\u20135, but I doubt that this explains the absence rate. There was not city data associated with the original dataset, so it is difficult to attempt to cross-reference specific dates with specific events.\\nI will create a simliar graph for 2014. I remember that the 2014 soccer World Cup was held in Brazil, so perhaps the time around the world cup will have a higher absence rate. I will plot a few holidays and then the dates surrounding the World Cup (which lasted for a month). Perhaps there is a slight increase in absence rate during the World Cup? The graph does not clearly indicate either was but I can compare the average absence rate during the World Cup to the average absence rate over the entire length of 2014.Mean absence rate during all of 2014\\\\ [1] 30.08656\\nMean absence rate during 2014 World Cup\\n[1] 30.79473 There is a small but not significant increase in the absence rate during the World Cup. It looks like in terms of time variability, there is no significant correlation based on the day of the year or the day of the month. There is one more time variation I want to look at, and that is day of the week. To remind myself of the small sample sizes on the weekend, I also show the appointment days of the week count. Finally, it looks like there might be some time variability in this data when it is broken down by weekday. Excluding Saturday and Sunday because of their small sample sizes, Monday and Friday have the highest absence rate and Tuesday has the lowest. I will alter the plot to show the absence rate as a relative percentage change from the overall mean absence rate. Based on this chart, patients are 6% more likely to miss an appointment on a Monday compared to the overall average absence rate, and 4% less likely to miss an appointment on a Tuesday. This is an actionable discovery for both patients and doctors! If patients want to keep their appointments, and if doctors want to make sure their patients show up, they should schedule their appointments during the middle three days of the week. Patient Age I wanted to move on from the time variability and look at the demographic data. In particular, I am interested in whether or not age is correlated with absence rate. My initial guess would be that the youngest and the oldest patients would tend to have lower absence rates. Meanwhile, those patients in the middle would generally be healthier and thus would feel more inclined to skip an appointment. (Everyone is convinced they are invincible in their 20s. In fact, this was one of the issues associated with the initial rollout of Obamacare. Too many young, healthy individuals did not believe they needed insurance and therefore did not sign up for healthcare.) First I will group patients by age and then visualize the average absence rates. The chart reveals the possibility of a negative relationship between age and absence rate. First though, it is clear that the outliers in the upper end of the data introduce substantial variance. As there are only 719 patients over 90 out of 300000 entries, I think I can filter out any ages over 90 without impacting the validating of the data while reducing the noise. I will use that filter and then improve the aesthetics of the graph. We can observe that the youngest ages have a relatively low absence rate with an exception for 1-year-olds. The absence rate then climbs for teenagers, peaking around age 20, before beginning a long, gradual decline to around 70, at which point the absence rate rises again slightly. I would like to know precisely which ages have the highest and lowest absence rates. Moreover, what is the pearson correlation between age and absence rate? Maximum absence rate of 40.21% occurs at age 18\\nMinimum absence rate of 20.25% occurs at age 72 Here are more actionable conclusions. Public health officials need to work on getting teenagers to show up to their appointments! This is expecially crucial because studies have shown that habits form very early and are hard to change later in life. A trend of going to the doctor while younger will likely persist as a patient ages and lead to better lifelong health. We can further bin the data in five years increments to highlight the \\u201cproblem years\\u201d, that is, the years with the highest missed appointment rate. I will plot the relative absence rate, or the average age group absence rate as a percentage relative to the average absence rate for all ages. Remember, in this plot, below the x-axis is better because it indicates that the age group has a lower rate of missed appointments than the average. Based on the chart, is it clear that the worst group in terms of absence rate is 15\\u201320 year-olds and the best group for attedance is 70\\u201375 year-olds. The visualization and the statistics are definitive when it comes to ages and missed appointments. The correlation between ages and absence rate is -0.86, which is strongly negative and indicates that as the age of the patient increases, statistically, that patient is less likely to miss a scheduled doctor\\u2019s appointment. Waiting Time After finding a strong relationship between age and absence rate, I will move on to examine other variables. I would like to search for a possible trend in absence rate by waiting time, or the time between when an appointment was scheduled, and when the appointment took place. My hypothesis is that patients with a longer waiting time will have a higher missed appointment rate because their condition has more time to change in the interim period. Moreover, I think that patients with a shorter waiting time are more likely to need urgent care or have a problem they deem to be pressing, and it would certainly be in their best interest to show up at the appointment. The graph and the correlation coefficient seem to demonstrate a lack of any relationship. However, thinking back on the histogram of waiting time from the initial exploration, the majority of the patients waited under 100 days. There are far fewer data points from people waiting more than 100 days, which is why the average absence rate for those times tends to be either 0% or 100%. If I limit the data to people with a waiting time less than 3 months (90 days), might there exist a relationship? Furthermore, I will group the data into 10 day segments. Patients with a waiting time over 90 days\\n1646 If the data is limited to those patients who scheduled fewer than 3 months in advance, there is a slight positive correlation between waiting time and absence rate. I tried to do some research on this subject in general, and found this from the Safety Net Dental Clinic: \\u201cExperience in many safety net dental programs also suggests that the incidence of broken appointments increases when appointments are scheduled more than three weeks in advance.\\u201d Based on my analysis, I concur. Patients who schedule less than 10 days out are 20% less likely to miss an appointment than those scheduling further out. The Safety Net Dental Clinic page mentions that often other life events get in the way when we schedule too far out, and although that may be a slightly different situation, the general pattern appears to hold in this case. Add a point to schedule as few days out as possible to the list of recommendations for patients looking to adhere to a scheduled appointment. SMS Reminders One of the most intriguing aspects of the dataset to me was the SMS (Short Message Service) text message reminder counts. Text message reminders have been implemented in many clinics because of their ease of use and low cost. Moreover, they have been shown to be effective in some situations. I want to examine this data to discover how or if SMS messages correlate with a failure to attend. I would think that patients who recieve SMS messages will be more likely to attend the appointment than those not receiving them. However, this could be complicated by the fact that younger patients are more likely to sign up for text message alerts, and as we have already seen, young individuals are more inclined to be record a no-show. Therefore, I will probably need to further look at who exactly is recieving the text messages in addition to the correlation between text messages and attendance at the appointment. The graph is somewhat surprising to me; one text message seems to decrease the absence rate while two increases the absence rate significantly. I would have expected more SMS reminders to be strongly correlated with a decrease in absence rate. Maybe the direction of this relationship is the opposite of what I thought. Instead of SMS messages persuading patients to attend a scheduled appointment, patients who are least likely to attend an appointment receive more text messages. One way to check for this would be to look at average SMS messages per appointment received by age. I expect there might be odd behavior at the ends of this graph (how young do children receive their first phone nowadays? Has the average 80-year old embraced text messages?). I expect that segmenting by age might reveal more nuances than the overall absence rate vs text messages. Sure enough, the average number of SMS reminders per appointment is greater among the younger ages. I looked through the data provider information, and I could not ascertain whether or not those SMS reminders were to the parent in the case of a very young child but that seems likely. From the statistics and the plot, there is a strong negative correlation between the number of SMS messages and the age of the patient. To further explore the absence rate vs text message, I will recreate the absence rate vs age graph, but this time, create separate curves for number of SMS reminders received. Looking at the graph, we can see that the absence rate versus age grouped by number of SMS reminders looks much the same for the 0 or 1 reminders. However, the graph for 2 reminders is much noisier. I suspect that is due to the lower sample size of people receiving two reminders (which was the maximum). In order to determine if there is an effect from text message reminders at a given age, I need to create a plot showing the difference in absence rates at each age between those receiving any reminder and those receiving no reminder. Indeed, it does appears that at a given age, the absence rate decreases with 1 or 2 text messages reminders. In fact, the average difference in absence rate holding age constant is -0.57%. That means that having receiving an SMS reminder at a given age decreases the chance that a patient will miss a scheduled appointment by 0.6%. This may seem small, but it could add up over millions of appointments. Moreover, there are certain ages when the effect of the SMS reminders is much greater. The reduction in missed appointments is as great as 2% at age ten and near 3% during other years. This reduction in the failure to attend rate is also what research into text message reminders has found. One study,conducted in Brazil, found: \\u201cThe nonattendance reduction rates for appointments at the four outpatient clinics studied were 0.82% (p = .590), 3.55% (p = .009), 5.75% (p = .022), and 14.49% (p = <.001).\\u201d Furthermore, the conclusion of this research was: \\u201cThe study results indicate that sending appointment reminders as text messages to patients\\u2019 cell phones is an effective strategy to reduce nonattendance rates. When patients attend their appointments, the facility providing care and the patients receiving uninterrupted care benefit.\\u201d Based on my analysis of the data, I must concur that at a given age, SMS reminders decrease the absence rate. Another valuable piece of information that could be quite easily implemented in the real world to improve outcomes! Patient Markers The final step in this exploratory analysis is to look at the absence rate by the condition for which the patient visited the doctor. I will group by the markers in the original data and then look at each average absence rate. There are considerable differences illustrated between those with different markers. Tuberculosis has the highest rate of absence, followed by alcoholism. However, these conditions also make up a small sample of the overall visits. Diabetes and hypertension have the lowest absence rate. Interestingly, patients marked as being part of the Bolsa Familia welfare program have a high rate of absences. This program rewards patients for attending appointments, which should induce patients to visit their doctor more often. Perhaps there are other factors at play here though. It could be that families on the welfare program cannot take the time to visit the doctor because of the opportunity cost of not working. To put the conditions in perspective, here is the percentage of all visits each condition makes up. The final plot I can make is of the conditions and the relative percentage difference in absence rates compared to the mean absence rate. Relative difference plots are helpful to me because it is possible to quickly identify if one variable is above or below the average. Again, for this particular plot, values below the x-axis are great because it means the absence rate is lower than the overall average. The differences between conditions here are stark. Again, the smaller sample sizes associated with alcoholism and tuberculosis must be taken into account, but it is clear that patients with diabetes and hypertension are more likely to attend an appointment. In fact, they are more than 15% less likely to miss an appointment then the average patient. This is an interesting result and I wonder if it is because of the constant need for care when treating both of these conditions. However, it would seem like all of the conditions require care on an ongoing basis. I would need more detailed condition information before I drew any conclusions about possible causes of the absence rates broken out by condition. I started this project off with a single simple question: what factors are most likely to determine whether or not a patient shows up to their scheduled doctor\\u2019s appointment? As the exploratory data analysis went on, I found this one question had branched into dozens. Does the number of SMS reminders correlate with absence rates? How about if we compare SMS reminders for a given age? Are there major variabilities that we can observe associated with holidays? I found my curiousity and interest in the dataset only grew as I delved further and further. Although at first it appeared there were few meaningful relationships, by grouping and segmenting the data, clear trends emerged. Keeping in mind that this dataset may not be representative of all countries and health care systems, the following are the most notable discoveries from the patient no-show data: 2. There is a slight positive correlation between absence rate and the day of the month and as the month progresses, the percentage of missed appointments rises moderately. 3. There is no correlation in the absence rate over the days of the year. 4. Excluding the weekends, the day of the week with the highest absence rate is Monday followed by Friday. Tuesday had the lowest rate of absences with appointments on Tuesday 4% more likely to be attended than those on the other days of the week. 5. The age of patients was demonstrated a strong negative linear correlation with absence rates with a correlation coefficient of -0.86. 6. There was a slight positive correlation between the rate of missed appointments and how many days in advance the patient scheduled the appointment. 7. When looking at the dataset as a whole, patients who recieved two SMS reminders were more likely to miss an appointment than those who recieved no reminders. However, the correlation between age and average text messages recieved was -0.80 meaning that younger patients, who were more likely to miss appointments overall, received more text messages. Subsequently, the effect of text messages reminders can only be revealed by looking at messages received at a given age. 8. Patients whose appointments were marked for tuberculosis and alcoholism were the most likely to miss the visit, while those coded for hypertension and diabetes were the least likely to miss their appointment. These are but a few briefs observations that can be gleaned from this dataset. Keeping in mind that correlations do not imply causations (more serious link for those interested), and that some of the groupings results in small sample sizes, there are actionable items that with appropriate further study, could be implemented in the health care system. The final aspect of the project was to revisit and refine three of my earlier plots. Primarily I am concerned with whether or not the visualizations correctly convey the information within the dataset. My secondary objective is of course aesthetics because what good is even the most informative chart if it is not presentable? Absence Rate vs. Day of the Year This first chart was notable to me because of the lack of relationships it revealed. It is the graph of absence rate versus day of the year in 2014 with holidays (and the World Cup) included. I would have initially believed that absence rates spiked around holidays and near the end of the year. However, no clear trend emerged from this plot. In order to improve the information content and the aesthetics, I altered some of the colors, changed the scale on the axis, added in the mean absence rate for reference, and made sure all labels were accurate. The conclusion to draw from this visualization is that there is no trend in rate of missed appointments over the year by day. Even on major public holidays (of which the World Cup may be the greatest!) there is no noticeable change in absence rate around or on the holiday. This was backed up by the correlation coefficient which showed no linear relationship between day of year and the failure to attend statistic. The overall mean, plotted as the horizontal black line, shows the absence rate for all patients was 30.24%. Women on average had a slightly lower rate of 29.87% and men had a slightly higher rate at 31.00%. The gender discrepancy can barely be picked out on the graph, but it is present. Absence Rate vs. Age by SMS Reminders The second crucial visualization is the absence rate versus age broken down by number of SMS reminders. When I initially looked at the data showing that overall, people who received 2 text message reminders had higher absences rates, I was a little skeptical. However, after some thought, I realized that people who received text messages were likely to be younger, and I had seen that the younger the patient, the higher the absence rate. Therefore, I decided to look at the effect of text messages reminders at each given age. Based on the resulting chart, I saw that indeed, SMS reminders do reduce the absence rate at a specific age. To make the point clearer in the plot below, I plotted the average absence rate vs age for those who received no text reminders and for those who received either 1 or 2 reminders. The original graph was very noisy, so I applied a moving average over 5 years. The 5 year window was a selection based on the bias-variance tradeoff because while I wanted a smoother plot, I did not want to introduce a high amount of bias into the averages. After improving the graph, it is possible to observe the trend of fewer absences at older ages and the effectiveness of SMS reminders in reducing the failure to attend rate. The main two takeaways from this visualization are that as age increases, the absence rate decreases, and, at a given age, patients who receive at least one SMS reminder miss fewer appointments. The age distribution of missed appointments was in line with what I had expected, although I would have initially guessed that children under 10 would have the low absence rates comparable to what was observed in those aged 40 and over. It was not surprising to me that patients in their teens and early 20s had the highest failure to attend rate and it is clear that more effort needs to be expended in ensuring that this age group shows up for their appointments. This is one application where text message reminders could be most useful. Looking at the chart, it is also possible to observe that the rate of no-shows for those receiving an SMS reminder is lower than that for those recieving no reminder at almost every age. Overall, for a given age, text message reminders resulted in absence rates 0.5% lower which could make a substantial impact on the scale on a national health care system. Absence Rate vs. Waiting Time The third vital graph from this analysis was the relative absence rate vs waiting time graph. This graph displays the absence rate relative the overall mean absence rate for patient groups based on how far in advance they scheduled their appointment. I initially grouped the waiting time, or the elasped days between when the appointment was made and when the appointment occurred, by groups of five days. I then plotted the relative absence rate compared to the overall average for each group. I was surprised to discover how much lower the absence rate was for those sceduling less than 10 days in advance. To improve the plot, I increased the resolution of the graph by narrowing the bin widths to groups of 3 days. I also decided to add in a model created with the LOESS, Local Regression, method of regression. I created a simple model and then used the prediction it generated for each age grouping to draw a curve. The graph reveals the crucial information that appointments scheduled shorter out tend to have higher attendance rates. The actionable takeaway for patients from this graph is to schedule appointments on a shorter time scale, preferentially less than 10 days in advance. The absence rate for those whose appointments were scheduled only 3 days or less in advance was nearly 30% lower relative to the overall absence rate. Moreover, the model shows that the trend in increasing absence rate with longer waiting times is nearly linear for the first month. After that point, the noise in the visualization increases, but the majority of groups with wait times over 12 days had a higher absence rate than the mean. Patients would be wise to schedule their appointments as soon as possible in order to follow through on visiting the doctor. Taken together, these three visualizations illustrate several pieces of advice for patients and doctors who have a mutal interest in driving the absence rate as low as possible:\\n1. Day of the year does not affect the absence rate even near the holidays.\\n2. Young adults are the most likely to miss their appointments and therefore will need extra prodding to attend an appointment. This prodding can come in the form of SMS reminders, which reduce the absence rate for a given age.\\n3. Patients should schedule appointments as few days in advance as possible. Ideally appointments would occur within 9 days of being scheduled to increase the chance of attendance. The primary reason I wanted to learn the tools of data analysis was in order to could extract meaningful information from the mounds of data generated in our modern world. I want to be able to take hundreds of thousands or even millions of data points and extract insights which can be implemented in the real world to improve human institutions, such as the medical system. Exploring the patient no-show dataset has been a small step towards developing that ability. Although the sheer amount of data appeared overwhelming at first, and there were no correlations that immediately stood out, by selectively grouping the data and adjusting the visualization parameters, I was able to discover several key relationships.\\nThe main difficulty I had was beginning the analysis. With so many variables, it was a struggle to decide where I should first concentrate. However, once I started grouping the data, I found more and more directions I could explore until I felt that I was satisfied with the extent to which I had unraveled the data. The trends and patterns in the datadictate the questions that we should ask of it. R is a tricky language to pick up, but once I understood the patterns and syntax, I enjoyed the level of control it gave me over the visualizations. I was frustrated at times trying to perfect parameters of a graph, but in the end, I think I am a better data analyst because I had to work through all the intricacies of the R language. The next step forward, now that the exploratory data analysis of the no-shows dataset is complete, is to perform confirmatory data analysis. Based on my initial observations, I could form several hypotheses and then use rigorous statistical methods to test those hypothesis. Exploratory data analysis can discover potential relationships, but it takes statistical testing to determine whether these correlations are statistically meaningful. Moreover, this dataset is an ideal candidate for using machine learning to create classifiers that would identify patients likely to be a no-show at an appointment. The objective would be to make a model that would take in as features patient demographics and conditions, and would return the likelihood that a patient would fail to attend a scheduled doctor\\u2019s appointment. If the model was accurate enough, it could then be implemented in the real-world by ensuring that patients most likely to miss a doctor\\u2019s visit receive additional persuasion. Moreover, with a more complete dataset, including city information or detailed demographics, more relationships could be discovered such as absence rate correlations with the weather or with access to public transportation. The initial exploratory data analysis of the doctor\\u2019s appointment no-show data has revealed numerous potential relationships. The dataset holds actionable information and this project demonstrates the benefits of not just collecting large amounts of data, but thoroughly analyzing it to find the correlations that could be used to improve patient outcomes and public health. Originally published at medium.com on August 8, 2017.\",\n          \"In his sweeping 2011 work The Better Angels of Our Nature: Why Violence has Declined, Steven Pinker makes the audacious claim, \\u201ctoday we may be living in the most peaceable era in our species\\u2019 existence.\\u201d To someone who pays even a little attention to constant news reports of violence, both local and international, this statement sounds ludicrous. The grandeur of this assertion should immediately raise our suspicions; as the science popularizer Carl Sagan often said, \\u201cextraordinary claims require extraordinary evidence.\\u201d Does Pinker have that evidence and does he interpret it correctly? Over the course of 800 pages, Pinker makes a convincing argument with numbers, figures, and references to many books and articles. Nonetheless, it\\u2019s critical that we don\\u2019t rely on a single argument to form our worldview. We must remember that data is never objective, and therefore we need to look at the data itself as well as multiple expert interpretations. In this article, we\\u2019ll examine the global violent conflict data and different stances on the same numbers. (As a note, Pinker\\u2019s work deals with all forms of violence from the individual \\u2014 homicides, abuse \\u2014 to the worldwide, but here we\\u2019ll focus on international violent conflicts \\u2014 war \\u2014 and leave the rest for future articles). This is Episode 2 of the Reality Project, an endeavor with the aim of becoming less wrong about the world with data. You can find all the articles here. (I\\u2019d like to emphasize I\\u2019m not trying to minimize the atrocities of war by presenting only data. It can be easy to lose track that we are talking about people when looking at numbers and every loss of human life is a tragedy.) Our primary sources of derived data on the more positive side will be The Better Angels of Our Nature by Steven Pinker and the \\u201cWar and Peace\\u201d page on Our World in Data. For a contrasting viewpoint, we\\u2019ll use \\u201cSteven Pinker is Wrong\\u201d by John Gray and \\u201cThe Decline of Conflict: What Do the Data Really Say?\\u201d by Pasquale Cirillo and Nassim Nicholas Taleb. You can find raw data for your own analysis at The Conflict Catalog or the World Bank. The first question we need to ask to assess violence today is how violent was the past. While extremely reliable data is difficult to come by, anthropologists use archaeological evidence to determine causes of death and therefore approximate the rates of violence in a society. These results are shown in the following graphs of percentages of deaths that come from violent conflict in prehistoric and non-state societies (from Our World in Data). The percentages of individuals killed by violence ranges from about 60% to less than 5% in both prehistoric and nonstate societies. Those numbers are almost meaningless until we put them in perspective with modern figures: The most relevant entries are the final two. The US and Europe from 1900\\u20131960, even with two world wars, saw less than 1% of their population perish in armed conflicts. In 2007, just 0.04% of deaths in the world were from international violence. If this data is correct, the world in 2007 was at least an order of magnitude safer than most prehistoric societies. We can also look at the same data in a slightly different manner, as the violent deaths per 100,000 citizens per year. Here we again see the same pattern of lower violence in modern times. On the left are non-state societies and on the right are state societies. Focusing on the bottom 3 rows of the right-hand table, we can see the strikingly low rates of violent death among modern societies, even accounting for two world wars. Again, it\\u2019s worth stressing the data here is not complete, but what is available suggests the following hypothesis: prehistoric and nonstate societies experienced much higher rates of violence than modern state societies. As Pinker and others (notably Jared Diamond) have made clear, the \\u201cnoble savage\\u201d idea is entirely false. People did not live peacefully with one another when they were organized in tribes and then become more violent as they were civilized, but the opposite: they were extremely violent in tribes and gradually became less so as larger civilizations were built and commerce started to connect the world (this highlights two of the drivers in the decline of global war \\u2014 trade and powerful states \\u2014 that we\\u2019ll look at shortly). Even accounting for the atrocities wrought by nation-states in the 20th century, rates of violent death appear to be lower now than at any previous point. It\\u2019s worth looking into the period of time for which we have the best data: the modern age. In particular, we\\u2019ll zoom in on the post-1945 era, a time that Pinker has classified as \\u201cThe Long Peace\\u201d. From this vantage point, things look very good. As noted by Pinker, since 1953, there have been zero conflicts between major world powers (the exception since WW2 was the Korean War) and there have been zero internationally recognized states that have gone out of existence through conquest since 1945. When we look at the data on battle deaths per 100,000 people in state-based conflicts, we see a significant decline since World War Two. Since the end of the second world war, the rate of deaths from all conflicts has decreased, with the most noticeable decline in conflicts between states. Most of the remaining battle deaths occur in civil conflicts, such as the tragic civil war in Syria that has been ongoing since 2011. Moreover, many of these civil wars involve foreign states, and the argument could be made major states have stopped fighting each other directly, but engage through other conflicts. One discouraging bit of news in contrast to the above graph is the number of overall state conflicts since the second world war: While the number of battle-related deaths (in the rate of people killed / 100,000 / year) has decreased, the total number of conflict is rising. This tells us that, contrary to what we might think as weapon technology advances, violent conflicts are resulting in fewer battle deaths. While, there are more civil conflicts around the globe, these tend to be less catastrophic than wars between major powers. Fortunately, the wars between major states have been on the decline \\u2014 even including the entire 20th century \\u2014 and show no signs of reversal. We can see this in the final chart of section showing the percent of the time major powers have been at war over the past 500 years (grouped into 25-year periods). There has been a clear decline in the percentage of years in which the major powers fought with one another since at least 1600. At one point the rate reached 100% \\u2014 indicating there was at least one conflict between world powers for the entire 25-year period. Contrast that with 2000, when there were 0 conflicts between world powers. The current period of 66 years without a conflict between major powers is the longest time since at least the Roman Empire. After an exploration of the data thus far, Pinker\\u2019s argument looks to be holding up: the rate of death in battle for modern state societies in war is much lower than prehistoric and non-state societies even accounting for the world wars. Furthermore, the major powers no longer fight one another and while the number of civil conflicts has increased, these result in fewer deaths than conflicts between major states. However, before we consider this case to be closed we need to talk about one fundamental issue. Consider the following two scenarios: Which is worse? The question depends on your observation point. On the ground, observing from the location of the conflict itself, the bar is much worse. An individual in the bar has a 1 in 2 chance of being killed. From an outsider\\u2019s vantage point the civil war is much worse with a total loss of life 10 times greater. This is one of the central arguments that Pinker\\u2019s critics make: measuring violence in rates ignores the absolute numbers which means it does not account for the actual amount of human suffering. One of Pinker\\u2019s harshest critics, Nassim Taleb, brings up the point of units in his response. He illustrates it with the following pair of plots: The graph on the left shows the raw number of casualties versus time for major conflicts. The graph on the right shows the rescaled version where the populations have been standardized across all times to have the same base population. As an example, if we have a population 100 society that suffers 2 deaths and a society with 1000 people that suffers 10, we would multiply 2 * 10 to get 20 deaths to account for the population difference. These figures encapsulate the rates/absolute numbers argument: the rates of death in violent conflicts have clearly been decreasing and are at a historical low, but the actual number of deaths has increased over time. Although, it\\u2019s worth pointing out the actual number has decreased since 1950. I don\\u2019t know if I have an answer for you on this problem of relative/absolute. As a utilitarian, I believe in bringing the greatest amount of total good to the greatest amount of people, and therefore, I see the total number of humans dead as the mark of how our civilization has failed. On the other side of the argument, a human alive today has less probability of being killed in a violent conflict than at any point in the past which is no doubt progress. I\\u2019ll let the facts stand and allow you to decide for yourself. Even if we don\\u2019t want to make a judgment on the overall outcome, it\\u2019s worth taking a look at the potential causes for the reduction in rates of violence. In particular, we might want to think about why the total number of deaths (in addition to the relative measures) have fallen since 1945. For this section, we will look at Steven Pinker\\u2019s ideas as outlined in The Better Angels of Our Nature. (This work is the best look at potential reasons why violence has declined \\u2014 partly because others refuse to concede this point). Pinker explains five forces, intended to account for the decline of violence on all scales, not just state-based conflicts, but we\\u2019ll focus on the first two which are most relevant to international conflict. The five historical forces behind the reduction in violence are: A powerful central government that can make and enforce laws using violence means that citizens are less likely to take punishment into their own hands, a concept known as the Leviathon theory. For all its romantic appeal, vigilante justice only leads to unending cycles of revenge violence. When citizens can count on a government to deliver fair punishments, they let the judicial system enforce the rules. Also, a state can prevent crime by making the penalty far greater than any potential payoff. Humans have the potential for both evil and good, and a strong central government with a fair criminal justice system with disincentives can steer them on the right path. A strong government alone is not enough for keeping the peace between nations though. On an international scale, the evidence suggests that democracies don\\u2019t fight each other possibly because of the ideals they share in common. Looking at some of the data, it\\u2019s clear that democracies just don\\u2019t fight one another directly anymore (although they may engage in proxy war). (For more on this concept see \\u201cDemocratic Peace Theory.\\u201d) The idea that one democratic nation will not fight another is good news as democracies on the rise in the long run worldwide as shown by Pinker: (One counterpoint is the recent decline in Democratic Index which measures not just the government but also freedom of the press and voting rights. Although the long-term trends are positive, the recent declines are worrying.) The theory of gentle commerce can be summed up in two statements: Why don\\u2019t you steal a loaf of bread from the grocery store? Although most people believe they don\\u2019t steal because it\\u2019s the wrong thing to do, the real reason is more mundane: in our society, the potential cost of getting caught stealing \\u2014 jail time and social ostracization \\u2014 are higher than the cost to purchase items. When people are provided a legal means to obtain their goods that is cheaper than violence, they will take the legal option. We explored the second point in \\u201cThe Disappearing Poor\\u201d, but it\\u2019s worth reiterating. Without exchanges in a market economy, all interactions between humans are zero-sum: you can steal something from me, but your gain is offset by my loss so humanity is no better off; the size of the economy stays the same. However, in an exchange of goods, both parties come out better ahead. Exchange also allows for specialization so different individuals/countries can make the goods for which they are best suited. Moreover, as trade continues, countries become dependent on one another because they no longer make all the goods they need. The end result of exchange is more plentiful goods at cheaper prices for all parties and improved international relations. Over the past several hundred years, we have slowly built up an international marketplace where all players are heavily dependent on one another. The US will not fight with another major power, not because she couldn\\u2019t defeat them, but because the economic losses would far outweigh the gains. As discussed by Pinker, countries that depend more on each other for trade are less likely to have a violent conflict controlling for other factors. In summary, the financial incentives have switched from war to trade for most nations. The Leviathon and gentle commerce actually work together \\u2014 a strong government is able to create markets that allow for safe exchanges and enforce laws regulating trade. In other words, once you have a strong state, trading is easier, and more trading between nations means they are less likely to attack one another. As trade between world powers has increased, conflicts between major powers has also decreased to 0 since 1953. There are doubtless other factors that have played a role in the decline of war between nations. We discussed external historical factors behind the decrease, but Pinker and others (John Horgan in The End of War) have also outlined internal factors, like The Better Angels of Our Nature : empathy, self-control, morals, reason. By examining the data and the reasons, it\\u2019s clear that the major powers no longer see war as a feasible choice. This is not necessarily because they are ethical, but because the economics of war no longer makes sense. At the end of the day, humans \\u2014 and nations \\u2014 are driven by incentives, and through international trade, we have built a world where the incentives favor peace. In a narrow point of view, Pinker is correct: rates of violent conflict have substantially decreased and there is reason to believe they are at the lowest point in human history (if only because data is limited). Also, at least among modern democracies, violence is no longer seen as the default option for solving problems as it has been for almost all of human history. That being said, it\\u2019s worth thinking about how Pinker\\u2019s argument made be misleading. The first point is that by focusing on rates rather than numbers, we are neglecting the actual human suffering. More people dying from war is more people dying from war even if the percentage is decreasing. The numbers can be used to support multiple conclusions depending on one\\u2019s argument. A second central issue is, especially for pre-historic societies, reliable data is hard to get. Most of the numbers for conflict rates and deaths in prehistoric societies come by examining archaeological sites and looking at artifacts for evidence of violent death. However, it could be possible these marks have been misinterpreted and anthropologists could not possibly have studied all prehistoric societies. Simple extrapolation what has been found is the only option, but it\\u2019s also important to not draw immature conclusions. The last major issue we\\u2019ll cover here (for more see Taleb\\u2019s article) is that we could be living in an anomalous age. Pinker calls the most recent 70 years of peace (no wars between modern nations), \\u201cThe Long Peace\\u201d, but it could be really only the temporary peace. Furthermore, as pointed out by Taleb, wars tend to be power law distributed, that is, they have long tails skewed to the right. The vast majority of deaths in armed conflicts are caused by a small number of the conflicts as can be seen below: One single conflict can completely outweigh all others in terms of deaths. Most of the armed conflicts since 1945 have fallen on the low end of the scale, but it only takes a single major battle to overshadow the peace since 1945. It\\u2019s possible a new conflict, one caused by global warming, resource shortages, or territorial disputes, could entirely undo the millennia of progress humans have made on violence reduction efforts. Initially, after reading Pinker\\u2019s book, I was convinced by his thesis. However, after taking the time to dig through the facts myself, a more nuanced picture emerged, one where sensationalist conflicts on both sides are not warranted. I see reasons for optimism: decreasing rates of deaths in armed conflicts mean a human alive today is less likely to die in battle than in recorded history, no battles between major powers since 1953, and a reduction in overall battle deaths since the 1950s, as well as reasons for pessimism: democracy and international trade, so important for reducing major conflicts, appear to be on a decline in recent years. Moreover, while major powers no longer fight directly with each other, they are involved through proxy wars. The exploration of violent conflict highlights the objective of the Reality Project. It\\u2019s not designed to be a feel-good effort but a fact-based endeavor, and, thus, any time I encounter an argument that is appealing to me this should only tell me to be more skeptical. The Reality Project has allegiances to no single interest, except getting at the actual statistics behind our world. It\\u2019s nice when the facts make us optimistic, but even when they don\\u2019t, it\\u2019s critical to understand the data so we can work to improve things. As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tag\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Data Science\",\n          \"Personal Development\",\n          \"Education\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "df = data[[\"Paragraph\",\"Tag\"]].copy()\n",
        "df = df.rename(columns = {\"Paragraph\":\"Blog\"})\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tag'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l09tFi2sHJYu",
        "outputId": "c0e7b45e-f4b6-4faa-fb0d-3f3e8022b52c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Data Science', 'Personal Development', 'Technology',\n",
              "       'Machine Learning', 'Programming', 'Education'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Since Dataset only contains 155 Blogs, To create more sample for model to train better we devide each blog to chunk size of 80 words with 20 overlapping owords"
      ],
      "metadata": {
        "id": "alg7Wd2PoArD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqjWpR2gR8k-",
        "outputId": "a4ed7647-8d7e-4a66-ee1a-f0d99d6ac5f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "def create_overlapping_segments(text, chunk_size=80, overlap=20):\n",
        "  \"\"\"\n",
        "    Function to create overlapping segments of text with specified chunk size and overlap.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input text to be segmented.\n",
        "        chunk_size (int): The desired chunk size in terms of number of words.\n",
        "        overlap (int): The overlap between adjacent chunks in terms of number of words.\n",
        "\n",
        "    Returns:\n",
        "        List of overlapping segments.\n",
        "  \"\"\"\n",
        "  tokens = word_tokenize(text)\n",
        "  segments = []\n",
        "  start = 0\n",
        "  end = chunk_size\n",
        "  while start < len(tokens):\n",
        "    segment = tokens[start:end]\n",
        "    segments.append(\" \".join(segment))\n",
        "    start += chunk_size - overlap\n",
        "    end = start + chunk_size\n",
        "  return segments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *This is Done to increase the Dataset Size*"
      ],
      "metadata": {
        "id": "tIvHw8UAfZZI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "SyGofyZgSryK"
      },
      "outputs": [],
      "source": [
        "segment = create_overlapping_segments(\"Sleep is important to a number of brain functions, including how nerve cells (neurons) communicate with each other. In fact, your brain and body stay remarkably active while you sleep. Recent findings suggest that sleep plays a housekeeping role that removes toxins in your brain that build up while you are awake.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "wcJVhgAPTBB_",
        "outputId": "25a4e2ff-0282-45e7-e55d-0c3e363da3f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sleep is important to a number of brain functions , including how nerve cells ( neurons ) communicate with each other . In fact , your brain and body stay remarkably active while you sleep . Recent findings suggest that sleep plays a housekeeping role that removes toxins in your brain that build up while you are awake .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "segment[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "N_6NndBOTnuQ"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "Y = []\n",
        "for i in range(len(df[\"Blog\"])):\n",
        "  segment =  create_overlapping_segments(df[\"Blog\"][i])\n",
        "  for sentence in segment:\n",
        "    X.append(sentence)\n",
        "    Y.append(df[\"Tag\"][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9nvLPSnUxFU",
        "outputId": "b1dc8a24-cb26-4433-f3d6-3dc3d36b6d70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7191"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "len(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Increased the SIze of Dataset"
      ],
      "metadata": {
        "id": "cTY7psnnoUO8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ihsvF8VOWkrh"
      },
      "outputs": [],
      "source": [
        "max_length = max(len(s) for s in X)\n",
        "padded_strings = [s.ljust(max_length) for s in X]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWxvdV-3UzIx",
        "outputId": "dfdc4e99-58d7-46e1-be55-adc21717b487"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7191"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "len(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjmzEoP7U0fh",
        "outputId": "50f7da3b-aa14-402f-911f-944f493637aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('In October 2020 , I was interviewed by DrivenData , an organization that hosts data science competitions for good , one of which I placed second in while teaching myself data science . My interview appeared condensed and edited on DrivenData  s blog and here are my full , unedited answers . Even if you don  t make it through this article  there  s a reason my responses were condensed I  d recommend checking out',\n",
              " 'Data Science')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "X[0],Y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dpoEHVwWU3jR",
        "outputId": "b5d84f00-b3aa-42e8-b3ab-1ff55bff8c00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Blog           Tag\n",
              "0  In October 2020 , I was interviewed by DrivenD...  Data Science\n",
              "1  it through this article  there  s a reason m...  Data Science\n",
              "2  to use in the best interests of our world . Wh...  Data Science\n",
              "3  models providing continuous , real-time operat...  Data Science\n",
              "4  field where the best decision for the environm...  Data Science"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0088a0a8-d253-4a38-ba05-158492718391\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Blog</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In October 2020 , I was interviewed by DrivenD...</td>\n",
              "      <td>Data Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it through this article  there  s a reason m...</td>\n",
              "      <td>Data Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>to use in the best interests of our world . Wh...</td>\n",
              "      <td>Data Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>models providing continuous , real-time operat...</td>\n",
              "      <td>Data Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>field where the best decision for the environm...</td>\n",
              "      <td>Data Science</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0088a0a8-d253-4a38-ba05-158492718391')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0088a0a8-d253-4a38-ba05-158492718391 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0088a0a8-d253-4a38-ba05-158492718391');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-91010eb5-73da-4301-9a3a-7a100fa59e4c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-91010eb5-73da-4301-9a3a-7a100fa59e4c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-91010eb5-73da-4301-9a3a-7a100fa59e4c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "new_df",
              "summary": "{\n  \"name\": \"new_df\",\n  \"rows\": 7191,\n  \"fields\": [\n    {\n      \"column\": \"Blog\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7175,\n        \"samples\": [\n          \"supervised machine learning task where the goal is to use historical loan application data to predict whether or not an applicant will repay a loan . During training , we provide our model with the features \\u2014 the variables describing a loan application \\u2014 and the label \\u2014 a binary 0 if the loan was repaid and a 1 if the loan was not repaid \\u2014 and the model learns a mapping from the features to the label . Then\",\n          \"we tell each other , and if we choose to focus on people and teams working to improve the world , we can not only believe in a better future , but work to make it a reality .\",\n          \"how they are relevant . Neil deGrasse Tyson is the best-known physicist in the world , not because he publishes the most brilliant papers , but because he translates tough concepts for a wide audience . Clear written and spoken communication skills are a major advantage that can not be taught in a classroom ! These once a week posts will usually be about data science and machine learning with a focus on real-world examples and metaphors . A good\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tag\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Data Science\",\n          \"Personal Development\",\n          \"Education\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "new_df = pd.DataFrame({\"Blog\": X, \"Tag\": Y})\n",
        "new_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LOdYynh7B4ap"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nh9Che-VDna",
        "outputId": "1e9649a2-6faa-43d4-c79a-dd4ff792ee74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data Science            2523\n",
              "Machine Learning        2520\n",
              "Personal Development     770\n",
              "Education                718\n",
              "Programming              382\n",
              "Technology               278\n",
              "Name: Tag, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "new_df[\"Tag\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Since Data is Imbalanced I Will use SMOTE for Oversampling ***"
      ],
      "metadata": {
        "id": "iNqxQlypfKHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVi2dkmmV7sD",
        "outputId": "bc4e7697-a9d6-44f9-bb4c-6e68a9ddd396"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Blog    0\n",
              "Tag     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "new_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "BAgg6txpWz1k"
      },
      "outputs": [],
      "source": [
        "new_df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aouvMTHyZZgM",
        "outputId": "0275fd56-8506-4ec0-83cf-f8b25b9e42c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Preprocessing Text Data\n",
        "- I am Lowering it, removing any punctuation,splitting them and using Lemmatizer"
      ],
      "metadata": {
        "id": "elJ2eDopo2QV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "oEi5lgVVW6KM"
      },
      "outputs": [],
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def clean_text(text):\n",
        "  sentence = text.lower()\n",
        "  sentence = re.sub(\"[^a-z0-9]\",' ',sentence)\n",
        "  sentence = sentence.split()\n",
        "  sentence = [lemmatizer.lemmatize(word) for word in sentence if word not in STOPWORDS]\n",
        "  sentence = \" \".join(sentence)\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Zhv_KRqNZ0GT"
      },
      "outputs": [],
      "source": [
        "new_df[\"Blog\"] = new_df[\"Blog\"].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "76lVIE_caDkg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x = new_df.Blog\n",
        "y = new_df.Tag\n",
        "x_train,x_test, y_train,y_test = train_test_split(x,y,test_size=0.1,random_state=42,stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXLKAVBabFOJ",
        "outputId": "7005eaa0-8ba3-4c3c-87c7-28763884ebc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6459,), (718,), (6459,), (718,))"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "cBSxoZpGfrfK"
      },
      "outputs": [],
      "source": [
        "from category_encoders import CountEncoder\n",
        "count_enc = CountEncoder(normalize=True)\n",
        "y_train_encoded = count_enc.fit_transform(y_train)\n",
        "y_test_encoded = count_enc.transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "PJjwi_M9g-A_"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFtZ6Or2f9UT",
        "outputId": "4c31b2e3-b038-40ab-b5e9-9c65d8aef924"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 4, 3, ..., 0, 0, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "y_train_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "t8BpNz79boHS"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from category_encoders import TargetEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline ## Pipeline to train the Model\n",
        "# Creating Pipeline TO Efficiently Train\n",
        "mulnb = ImbPipeline([\n",
        "    ('vect',CountVectorizer()), ## Using Count Vectorizer and TfidfTransformer to create Vectors from Text\n",
        "    ('tfidf',TfidfTransformer()),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf',MultinomialNB())\n",
        "])\n",
        "mulnb.fit(x_train,y_train_encoded)\n",
        "y_pred = mulnb.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3N9p-dncki7",
        "outputId": "432ceec4-513d-4ed0-d663-95006b20df32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8774373259052924"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "accuracy_score(y_pred,y_test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_test_encoded, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-m-gdkXJ9Nk",
        "outputId": "fea94356-4e42-4198-92cf-ec22245f0a40"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.81      0.85       251\n",
            "           1       0.92      0.92      0.92        72\n",
            "           2       0.91      0.92      0.92       252\n",
            "           3       0.82      0.91      0.86        77\n",
            "           4       0.84      0.84      0.84        38\n",
            "           5       0.62      0.93      0.74        28\n",
            "\n",
            "    accuracy                           0.88       718\n",
            "   macro avg       0.84      0.89      0.86       718\n",
            "weighted avg       0.88      0.88      0.88       718\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "7TAq-M3OhsVy"
      },
      "outputs": [],
      "source": [
        "xgboost = ImbPipeline([\n",
        "    ('vect',CountVectorizer()),\n",
        "    ('tfidf',TfidfTransformer()),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf',XGBClassifier())\n",
        "])\n",
        "xgboost.fit(x_train,y_train_encoded)\n",
        "y_pred = xgboost.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "giafdR29ilXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef490c6a-900b-4137-eff0-f706a7680651"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8147632311977716"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "accuracy_score(y_pred,y_test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_test_encoded, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BajOAtbuKOxG",
        "outputId": "22104943-b0e2-475b-f10e-675480fea278"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.81      0.79       251\n",
            "           1       0.83      0.82      0.83        72\n",
            "           2       0.86      0.91      0.89       252\n",
            "           3       0.73      0.75      0.74        77\n",
            "           4       1.00      0.63      0.77        38\n",
            "           5       0.73      0.39      0.51        28\n",
            "\n",
            "    accuracy                           0.81       718\n",
            "   macro avg       0.82      0.72      0.76       718\n",
            "weighted avg       0.82      0.81      0.81       718\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "random_forest = ImbPipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "random_forest.fit(x_train, y_train_encoded)\n",
        "y_pred = random_forest.predict(x_test)"
      ],
      "metadata": {
        "id": "hSt344D4-4HL"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_pred,y_test_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgntA8DceuHs",
        "outputId": "69ae7cc4-c1e6-46d9-bb71-476279b3ae80"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8119777158774373"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_test_encoded, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsMCLDCcKQ-R",
        "outputId": "50b0ef1c-ae9a-4099-a128-740fd17644e2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.85      0.80       251\n",
            "           1       0.87      0.74      0.80        72\n",
            "           2       0.85      0.91      0.88       252\n",
            "           3       0.78      0.73      0.75        77\n",
            "           4       1.00      0.66      0.79        38\n",
            "           5       0.78      0.25      0.38        28\n",
            "\n",
            "    accuracy                           0.81       718\n",
            "   macro avg       0.84      0.69      0.73       718\n",
            "weighted avg       0.82      0.81      0.81       718\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svm_pipeline = ImbPipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf', SVC(random_state=42))\n",
        "])\n",
        "svm_pipeline.fit(x_train, y_train_encoded)\n",
        "y_pred = svm_pipeline.predict(x_test)"
      ],
      "metadata": {
        "id": "hI1PEum0rQBF"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_pred,y_test_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJh56re6rTak",
        "outputId": "bb68cf3d-a274-4ecd-b3e6-fb9d0f40c6d3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8635097493036211"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_test_encoded, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eORwYW6rWIi",
        "outputId": "674ad4b1-78db-4afe-c6a1-fb2f87a83507"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.93      0.85       251\n",
            "           1       0.91      0.85      0.88        72\n",
            "           2       0.93      0.91      0.92       252\n",
            "           3       0.91      0.81      0.86        77\n",
            "           4       1.00      0.66      0.79        38\n",
            "           5       0.90      0.32      0.47        28\n",
            "\n",
            "    accuracy                           0.86       718\n",
            "   macro avg       0.90      0.75      0.79       718\n",
            "weighted avg       0.87      0.86      0.86       718\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "hybrid_classifier = VotingClassifier(\n",
        "    estimators=[('svm', svm_pipeline), ('xgboost', xgboost)],\n",
        "    voting='hard'\n",
        ")\n",
        "hybrid_classifier.fit(x_train, y_train_encoded)\n",
        "y_pred = hybrid_classifier.predict(x_test)"
      ],
      "metadata": {
        "id": "Y0f2W4ygrWqY"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_pred,y_test_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSb7tpfJr8pJ",
        "outputId": "50ce3fc4-674d-45de-8f58-013571438865"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8398328690807799"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_test_encoded, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH9u0ZNSsBKp",
        "outputId": "b31e8821-0339-431c-fb28-467cd564dd84"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.95      0.82       251\n",
            "           1       0.87      0.85      0.86        72\n",
            "           2       0.94      0.88      0.91       252\n",
            "           3       0.98      0.69      0.81        77\n",
            "           4       1.00      0.58      0.73        38\n",
            "           5       0.86      0.21      0.34        28\n",
            "\n",
            "    accuracy                           0.84       718\n",
            "   macro avg       0.90      0.69      0.75       718\n",
            "weighted avg       0.86      0.84      0.83       718\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- So Multinomial Naive Bayes Algorithm Performed Better Than Other Model\n"
      ],
      "metadata": {
        "id": "euvWMWJPJdS1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nVaFAE_dJlLn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}